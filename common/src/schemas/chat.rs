use std::collections::HashMap;

use serde::{Deserialize, Serialize};
use utoipa::ToSchema;

#[derive(Clone, Deserialize, ToSchema, Serialize, Debug)]
pub struct ChatRequest {
    /// The model ID to use for the chat.
    #[schema(example = "gpt3.5-turbo")]
    pub model: String,

    /// A list of messages comprising the conversation so far.
    #[schema(example = "[{\"role\": \"user\", \"content\": \"What is Deep Learning?\"}]")]
    pub messages: Vec<Message>,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency
    /// in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    /// Default: 0.0
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(nullable = true, example = "1.0")]
    pub frequency_penalty: Option<f32>,

    /// Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object that maps tokens
    /// (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically,
    /// the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model,
    /// but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should
    /// result in a ban or exclusive selection of the relevant token.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(nullable = true)]
    pub logit_bias: Option<HashMap<i32, i32>>,

    /// Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each
    /// output token returned in the content of message.
    /// Default: false
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(example = "false")]
    pub logprobs: Option<bool>,

    /// An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with
    /// an associated log probability. logprobs must be set to true if this parameter is used.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(nullable = true, example = "5")]
    pub top_logprobs: Option<u32>,

    /// The maximum number of tokens that can be generated in the chat completion.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(nullable = true, example = "32")]
    pub max_tokens: Option<u32>,

    /// How many chat completion choices to generate for each input message. Note that you will be charged based on the
    /// number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
    /// Default: 1
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(nullable = true, example = "2")]
    pub n: Option<u32>,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far,
    /// increasing the model's likelihood to talk about new topics
    /// Default: 0.0
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(nullable = true, example = 0.1)]
    pub presence_penalty: Option<f32>,

    /// Response format constraints for the generation.
    ///
    /// NOTE: A request can use `response_format` OR `tools` but not both.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(nullable = true, example = "null")]
    pub response_format: Option<GrammarType>,

    /// This feature is in Beta. If specified, our system will make a best effort to sample deterministically,
    /// such that repeated requests with the same seed and parameters should return the same result.Determinism is
    /// not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(nullable = true, example = 42)]
    pub seed: Option<u64>,

    /// Up to 4 sequences where the API will stop generating further tokens.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(nullable = true, example = "null")]
    pub stop: Option<Vec<String>>,

    #[serde(default = "bool::default")]
    pub stream: bool,

    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stream_options: Option<StreamOptions>,

    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while
    /// lower values like 0.2 will make it more focused and deterministic.
    /// We generally recommend altering this or `top_p` but not both.
    /// Default: 1.0
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(nullable = true, example = 1.0)]
    pub temperature: Option<f32>,

    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the
    /// tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(nullable = true, example = 0.95)]
    pub top_p: Option<f32>,
}

#[derive(Clone, Deserialize, ToSchema, Serialize, Debug)]
pub struct StreamOptions {
    include_usage: bool,
}

#[derive(Clone, Deserialize, ToSchema, Serialize, Debug)]
pub struct Message {
    #[schema(example = "user")]
    pub role: String,

    #[schema(example = "Hello, how are you?")]
    pub content: String,

    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(example = "Viv")]
    name: Option<String>,
}

#[derive(Clone, Debug, Deserialize, ToSchema, Serialize)]
#[serde(tag = "type", content = "value")]
pub enum GrammarType {
    /// A string that represents a [JSON Schema](https://json-schema.org/).
    ///
    /// JSON Schema is a declarative language that allows to annotate JSON documents
    /// with types and descriptions.
    #[serde(rename = "json")]
    #[serde(alias = "json_object")]
    #[schema(example = json ! ({"properties": {"location":{"type": "string"}}}))]
    Json(serde_json::Value),

    #[serde(rename = "regex")]
    Regex(String),
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Debug, Default)]
pub struct ChatCompletion {
    /// A unique identifier for the chat completion.
    pub id: String,

    /// The object type, which is always "chat.completion".
    #[schema(example = "chat.completion")]
    pub object: String,

    /// The Unix timestamp (in seconds) of when the chat completion was created.
    #[schema(example = "1700000000")]
    pub created: u64,

    /// The model used for the chat completion.
    #[schema(example = "gpt3.5-turbo")]
    pub model: String,

    /// This fingerprint represents the backend configuration that the model runs with.
    /// Can be used in conjunction with the seed request parameter to understand when
    /// backend changes have been made that might impact determinism.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub system_fingerprint: Option<String>,

    /// A list of chat completion choices. Can be more than one if `n` is greater than 1.
    pub choices: Vec<ChatCompletionChoice>,

    /// Usage statistics for the completion request.
    pub usage: Usage,
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Debug)]
pub struct ChatCompletionChoice {
    pub index: u32,
    pub message: ChatCompletionMessage,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub logprobs: Option<ChatCompletionLogprobs>,
    pub finish_reason: String,
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Debug)]
pub struct ChatCompletionMessage {
    pub role: String,
    pub content: String,
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Debug)]
pub struct ChatCompletionLogprobs {
    pub content: Vec<ChatCompletionLogprob>,
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Debug)]
pub struct ChatCompletionLogprob {
    pub token: String,
    pub logprob: f32,
    pub top_logprobs: Vec<ChatCompletionTopLogprob>,
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Debug)]
pub struct ChatCompletionTopLogprob {
    pub token: String,
    pub logprob: f32,
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Default, Debug)]
pub struct Usage {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub total_tokens: u32,
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Debug)]
pub struct ChatCompletionChunk {
    /// A unique identifier for the chat completion.
    pub id: String,

    /// The object type, which is always "chat.completion".
    #[schema(example = "chat.completion")]
    pub object: String,

    /// The Unix timestamp (in seconds) of when the chat completion was created.
    #[schema(example = "1700000000")]
    pub created: u64,

    /// The model used for the chat completion.
    #[schema(example = "gpt3.5-turbo")]
    pub model: String,

    /// This fingerprint represents the backend configuration that the model runs with.
    /// Can be used in conjunction with the seed request parameter to understand when
    /// backend changes have been made that might impact determinism.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub system_fingerprint: Option<String>,

    /// A list of chat completion choices. Can be more than one if `n` is greater than 1.
    pub choices: Vec<ChatCompletionChunkChoice>,

    /// Usage statistics for the entire completion request.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub usage: Option<Usage>,
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Debug)]
pub struct ChatCompletionChunkChoice {
    pub index: u32,
    pub delta: ChatCompletionDelta,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub logprobs: Option<ChatCompletionLogprobs>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub finish_reason: Option<String>,
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Debug)]
#[serde(untagged)]
pub enum ChatCompletionDelta {
    Chat(TextMessage),
    Tool(ToolCallDelta),
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Debug, PartialEq)]
pub struct TextMessage {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(example = "user")]
    pub role: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(example = "Hi, ")]
    pub content: Option<String>,
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Debug, PartialEq)]
pub struct ToolCallDelta {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schema(example = "assistant")]
    pub role: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tool_calls: Option<DeltaToolCall>,
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Debug, PartialEq)]
pub struct DeltaToolCall {
    pub index: u32,
    pub id: String,
    pub r#type: String,
    pub function: Function,
}

#[derive(Clone, Deserialize, Serialize, ToSchema, Debug, PartialEq)]
pub struct Function {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    pub arguments: String,
}
